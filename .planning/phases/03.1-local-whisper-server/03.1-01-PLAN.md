---
phase: 03.1-local-whisper-server
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/api/whisper.ts
  - lib/pipeline/store.ts
  - app.json
  - .env.example
  - scripts/whisper-server.sh
autonomous: false
user_setup:
  - service: whisper.cpp
    why: "Local speech-to-text server (replaces OpenAI Whisper API)"
    prerequisites:
      - task: "Install cmake and ffmpeg"
        command: "brew install cmake ffmpeg"
      - task: "Clone and build whisper.cpp from source"
        commands:
          - "git clone https://github.com/ggml-org/whisper.cpp.git ~/whisper.cpp"
          - "cd ~/whisper.cpp && cmake -B build -DWHISPER_BUILD_SERVER=ON && cmake --build build -j --config Release"
        notes: "Homebrew's whisper-cpp formula disables the server binary, so we must build from source"
      - task: "Download the small.en model (488 MB)"
        command: "cd ~/whisper.cpp && sh ./models/download-ggml-model.sh small.en"
    verification: "~/whisper.cpp/build/bin/whisper-server --help should show available flags"

must_haves:
  truths:
    - "whisper.ts uses local server endpoint in dev mode, OpenAI endpoint in production"
    - "Authorization header is skipped when using local server"
    - "Pipeline store does not require OpenAI key when using local server"
    - "App allows local HTTP networking for dev client builds"
    - "A convenience script exists to start the whisper server"
  artifacts:
    - path: "lib/api/whisper.ts"
      provides: "Environment-aware Whisper transcription"
      contains: "__DEV__"
    - path: "lib/pipeline/store.ts"
      provides: "Pipeline with conditional key retrieval"
      contains: "isLocalWhisper"
    - path: "app.json"
      provides: "NSAllowsLocalNetworking for dev client builds"
      contains: "NSAllowsLocalNetworking"
    - path: "scripts/whisper-server.sh"
      provides: "Convenience script to start whisper-server"
    - path: ".env.example"
      provides: "Updated docs noting OpenAI key is optional with local server"
  key_links:
    - from: "lib/api/whisper.ts"
      to: "http://127.0.0.1:8080/v1/audio/transcriptions"
      via: "__DEV__ conditional endpoint"
      pattern: "__DEV__.*127\\.0\\.0\\.1"
    - from: "lib/pipeline/store.ts"
      to: "lib/api/whisper.ts"
      via: "transcribeAudio import"
      pattern: "import.*transcribeAudio.*from.*api/whisper"
    - from: "lib/pipeline/store.ts"
      to: "lib/api/config.ts"
      via: "conditional getOpenAIKey call"
      pattern: "isLocalWhisper|__DEV__"
---

<objective>
Make the app use a local whisper.cpp server for audio transcription during development, eliminating OpenAI API fees. The app falls back to OpenAI's cloud API in production.

Purpose: Local transcription removes the dependency on OpenAI API keys for development and testing. The whisper-server exposes an OpenAI-compatible endpoint, so the change is minimal -- just endpoint URL and auth header logic.

Output: Modified `lib/api/whisper.ts` with environment-aware endpoint, updated pipeline store that skips OpenAI key retrieval in dev mode, `app.json` with local networking permission, a convenience startup script, and updated `.env.example` docs.
</objective>

<execution_context>
@/Users/damienanselmi/.claude/get-shit-done/workflows/execute-plan.md
@/Users/damienanselmi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03.1-local-whisper-server/RESEARCH.md
@lib/api/whisper.ts
@lib/api/config.ts
@lib/api/errors.ts
@lib/pipeline/store.ts
@app.json
@.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Make whisper.ts endpoint configurable and update pipeline store</name>
  <files>
    - lib/api/whisper.ts
    - lib/pipeline/store.ts
  </files>
  <action>
Modify `lib/api/whisper.ts` to use local whisper-server in development:

1. Replace the hardcoded `WHISPER_ENDPOINT` constant with an environment-aware selection:
```typescript
const WHISPER_ENDPOINT = __DEV__
  ? 'http://127.0.0.1:8080/v1/audio/transcriptions'
  : 'https://api.openai.com/v1/audio/transcriptions';
```

2. Add a helper constant to check if we're using the local server:
```typescript
const isLocalWhisper = WHISPER_ENDPOINT.startsWith('http://127.0.0.1');
```

3. Export `isLocalWhisper` so the pipeline store can use it to skip key retrieval.

4. In the `transcribeAudio` function, make the `apiKey` parameter optional (it's not needed for local server):
   - Change signature to `apiKey?: string`
   - Only include the `Authorization` header when `!isLocalWhisper` AND `apiKey` is provided:
```typescript
const headers: Record<string, string> = {};
if (!isLocalWhisper && apiKey) {
  headers.Authorization = `Bearer ${apiKey}`;
}
```

5. Keep the `model: 'whisper-1'` parameter -- whisper-server ignores it, OpenAI requires it.

6. Keep `fieldName: 'file'` and `mimeType: 'audio/m4a'` unchanged -- both servers expect these.

7. Keep all existing error handling (TranscriptionError with status codes). The local server returns standard HTTP status codes too.

8. Update the JSDoc to document the dual-mode behavior.

Then modify `lib/pipeline/store.ts`:

1. Import `isLocalWhisper` from `@/lib/api/whisper`.

2. In `startProcessing()`, make the OpenAI key retrieval conditional:
```typescript
// Stage 3: Transcribing
let transcript: string;
if (isLocalWhisper) {
  // Local whisper-server: no API key needed
  transcript = await transcribeAudio(audioUri);
} else {
  const openaiKey = await getOpenAIKey();
  transcript = await transcribeAudio(audioUri, openaiKey);
}
set({ stage: 'structuring', progress: 0.75 });
```

3. Keep the `getAnthropicKey()` call unchanged -- Claude API still needs its key.

4. Keep `getOpenAIKey` in the imports (still used in the else branch), but the import of `isLocalWhisper` is new.
  </action>
  <verify>
- `npm run lint` passes (no TypeScript errors)
- `lib/api/whisper.ts` contains `__DEV__` for endpoint selection
- `lib/api/whisper.ts` exports `isLocalWhisper`
- `lib/api/whisper.ts` has optional `apiKey?: string` parameter
- `lib/api/whisper.ts` conditionally includes Authorization header
- `lib/pipeline/store.ts` imports `isLocalWhisper` from whisper module
- `lib/pipeline/store.ts` conditionally calls `getOpenAIKey()` only when `!isLocalWhisper`
  </verify>
  <done>
whisper.ts uses `http://127.0.0.1:8080` in dev mode and `https://api.openai.com` in production. Auth header is only sent to OpenAI. Pipeline store skips OpenAI key retrieval when using local server.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add local networking permission, startup script, and update docs</name>
  <files>
    - app.json
    - scripts/whisper-server.sh
    - .env.example
  </files>
  <action>
1. Modify `app.json` to add `NSAllowsLocalNetworking` under `expo.ios.infoPlist`:
```json
{
  "expo": {
    "ios": {
      "supportsTablet": true,
      "bundleIdentifier": "com.wechef.app",
      "infoPlist": {
        "NSAppTransportSecurity": {
          "NSAllowsLocalNetworking": true
        }
      }
    }
  }
}
```
This is a safety measure for dev client builds. Expo Go in dev mode generally allows HTTP localhost, but dev client builds may enforce ATS. This setting only affects local networking (127.0.0.1, LAN IPs) and does not weaken security for external connections. Keep all other existing fields in app.json unchanged.

2. Create `scripts/whisper-server.sh` -- a convenience script to start the local whisper server:
```bash
#!/usr/bin/env bash
# Start local whisper.cpp server for WeChef development.
# Prerequisites: brew install cmake ffmpeg
# First-time setup:
#   git clone https://github.com/ggml-org/whisper.cpp.git ~/whisper.cpp
#   cd ~/whisper.cpp && cmake -B build -DWHISPER_BUILD_SERVER=ON
#   cmake --build build -j --config Release
#   sh ./models/download-ggml-model.sh small.en

set -euo pipefail

WHISPER_DIR="${WHISPER_DIR:-$HOME/whisper.cpp}"
MODEL="${WHISPER_MODEL:-$WHISPER_DIR/models/ggml-small.en.bin}"
PORT="${WHISPER_PORT:-8080}"
THREADS="${WHISPER_THREADS:-4}"

# Verify whisper-server binary exists
if [ ! -f "$WHISPER_DIR/build/bin/whisper-server" ]; then
  echo "Error: whisper-server not found at $WHISPER_DIR/build/bin/whisper-server"
  echo ""
  echo "Build it with:"
  echo "  git clone https://github.com/ggml-org/whisper.cpp.git $WHISPER_DIR"
  echo "  cd $WHISPER_DIR && cmake -B build -DWHISPER_BUILD_SERVER=ON"
  echo "  cmake --build build -j --config Release"
  exit 1
fi

# Verify model exists
if [ ! -f "$MODEL" ]; then
  echo "Error: Model not found at $MODEL"
  echo ""
  echo "Download it with:"
  echo "  cd $WHISPER_DIR && sh ./models/download-ggml-model.sh small.en"
  exit 1
fi

# Verify ffmpeg is installed (required for --convert flag)
if ! command -v ffmpeg &> /dev/null; then
  echo "Error: ffmpeg not found. Install with: brew install ffmpeg"
  exit 1
fi

echo "Starting whisper-server on port $PORT..."
echo "Model: $MODEL"
echo "Threads: $THREADS"
echo ""

exec "$WHISPER_DIR/build/bin/whisper-server" \
  --model "$MODEL" \
  --host 127.0.0.1 \
  --port "$PORT" \
  --inference-path "/v1/audio/transcriptions" \
  --convert \
  --threads "$THREADS" \
  --print-progress
```

Make the script executable: `chmod +x scripts/whisper-server.sh`

Note: Use `exec` so the server process replaces the shell (clean signal handling, Ctrl+C works).

3. Update `.env.example` to document that OpenAI key is optional with local whisper server:
```
# WeChef Environment Variables
# Copy this file to .env and fill in your API keys

# API Keys (for development only - use SecureStore in production)
# These keys are bundled into the app - use with caution

# OpenAI API Key (for Whisper audio transcription)
# OPTIONAL in development when using local whisper-server (see scripts/whisper-server.sh)
# Required in production builds
# Get from: https://platform.openai.com/api-keys
EXPO_PUBLIC_OPENAI_API_KEY=sk-...

# Anthropic API Key (for Claude recipe structuring)
# Get from: https://console.anthropic.com/settings/keys
EXPO_PUBLIC_ANTHROPIC_API_KEY=sk-ant-...
```
  </action>
  <verify>
- `app.json` contains `NSAllowsLocalNetworking: true` under `expo.ios.infoPlist.NSAppTransportSecurity`
- `app.json` preserves all existing fields (plugins, scheme, android, etc.)
- `scripts/whisper-server.sh` exists and is executable (`ls -la scripts/whisper-server.sh` shows `x` permission)
- `scripts/whisper-server.sh` contains `--inference-path "/v1/audio/transcriptions"` and `--convert`
- `.env.example` mentions local whisper-server and marks OpenAI key as OPTIONAL in dev
- `npm run lint` still passes
  </verify>
  <done>
app.json has NSAllowsLocalNetworking for dev client builds. Startup script at scripts/whisper-server.sh with validation and configurable env vars. .env.example updated to reflect OpenAI key is optional in dev.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify local whisper transcription works end-to-end</name>
  <what-built>
The app now uses a local whisper.cpp server for transcription in development mode. whisper.ts points to http://127.0.0.1:8080 when __DEV__ is true, skips the Authorization header, and the pipeline store skips OpenAI key retrieval. A convenience script starts the server with correct flags.
  </what-built>
  <how-to-verify>
**Pre-requisites (one-time setup):**
1. Install prerequisites: `brew install cmake ffmpeg`
2. Clone and build whisper.cpp:
   ```
   git clone https://github.com/ggml-org/whisper.cpp.git ~/whisper.cpp
   cd ~/whisper.cpp
   cmake -B build -DWHISPER_BUILD_SERVER=ON
   cmake --build build -j --config Release
   ```
3. Download the model (488 MB):
   ```
   cd ~/whisper.cpp && sh ./models/download-ggml-model.sh small.en
   ```

**Test the server directly:**
1. Start the server: `./scripts/whisper-server.sh`
2. In a separate terminal, test with curl using any .m4a audio file:
   ```
   curl http://127.0.0.1:8080/v1/audio/transcriptions \
     -F file="@/path/to/test-audio.m4a" \
     -F response_format="json"
   ```
3. Verify response is JSON: `{"text": "..."}`

**Test via the app (requires real audio file):**
1. Keep whisper-server running
2. Start the app: `npx expo start`
3. Trigger processing with a URL (download/extract are still mocked, so transcription will fail with mock audio URI -- this is expected)
4. To fully test, temporarily modify `mockExtract()` in `lib/pipeline/mock-api.ts` to return a real audio file URI on the simulator filesystem

**Code verification:**
1. Check `lib/api/whisper.ts` -- endpoint switches based on `__DEV__`
2. Check `lib/pipeline/store.ts` -- no `getOpenAIKey()` call in dev mode
3. Check `app.json` -- has `NSAllowsLocalNetworking`
  </how-to-verify>
  <resume-signal>Type "approved" if local transcription works, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:
1. `lib/api/whisper.ts` uses `__DEV__` for endpoint selection
2. `lib/api/whisper.ts` conditionally includes Authorization header
3. `lib/pipeline/store.ts` skips OpenAI key retrieval when using local server
4. `app.json` has `NSAllowsLocalNetworking: true`
5. `scripts/whisper-server.sh` exists, is executable, starts server correctly
6. `.env.example` documents local server option
7. No TypeScript/lint errors: `npm run lint`
8. App builds: `npx expo export --platform ios`
9. curl test against local server returns transcribed text
</verification>

<success_criteria>
- whisper.ts uses local endpoint in dev, OpenAI in production
- No OpenAI key required for development transcription
- whisper-server startup script works with proper validation
- App builds without errors
- Local server transcribes audio via curl test
</success_criteria>

<output>
After completion, create `.planning/phases/03.1-local-whisper-server/03.1-01-SUMMARY.md`
</output>
